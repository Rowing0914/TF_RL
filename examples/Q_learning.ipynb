{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from common.utils import AnnealingSchedule\n",
    "from common.params import Parameters\n",
    "from common.wrappers import DiscretisedEnv\n",
    "from common.visualise import plot_Q_values\n",
    "from agents.Q_learning_train import Q_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Params for CartPole Environment\n",
      "[Episode 0] - Mean time over last 100 episodes was 13.0 frames.\n",
      "[Episode 100] - Mean time over last 100 episodes was 23.18 frames.\n",
      "[Episode 200] - Mean time over last 100 episodes was 22.13 frames.\n",
      "[Episode 300] - Mean time over last 100 episodes was 28.28 frames.\n",
      "[Episode 400] - Mean time over last 100 episodes was 36.27 frames.\n",
      "[Episode 500] - Mean time over last 100 episodes was 40.59 frames.\n",
      "[Episode 600] - Mean time over last 100 episodes was 56.22 frames.\n",
      "[Episode 700] - Mean time over last 100 episodes was 86.64 frames.\n",
      "[Episode 800] - Mean time over last 100 episodes was 137.56 frames.\n",
      "Ran 899 episodes. Solved after 799 trials\n"
     ]
    }
   ],
   "source": [
    "# DiscretisedEnv\n",
    "env = DiscretisedEnv(gym.make('CartPole-v0'))\n",
    "\n",
    "# hyperparameters\n",
    "n_episodes = 2000\n",
    "goal_duration = 190\n",
    "all_rewards = list()\n",
    "durations = collections.deque(maxlen=100)\n",
    "params = Parameters(mode=\"CartPole\")\n",
    "Epsilon = AnnealingSchedule(start=params.epsilon_start, end=params.epsilon_end, decay_steps=params.decay_steps)\n",
    "Alpha = AnnealingSchedule(start=params.epsilon_start, end=params.epsilon_end, decay_steps=params.decay_steps)\n",
    "agent = Q_Agent(env, params)\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    current_state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    duration = 0\n",
    "\n",
    "    # one episode of q learning\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = agent.choose_action(current_state, Epsilon.get_value(episode))\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        agent.update(current_state, action, reward, new_state, Alpha.get_value(episode))\n",
    "        current_state = new_state\n",
    "        duration += 1\n",
    "\n",
    "    # mean duration of last 100 episodes\n",
    "    durations.append(duration)\n",
    "    all_rewards.append(duration)\n",
    "    mean_duration = np.mean(durations)\n",
    "\n",
    "    # check if our policy is good\n",
    "    if mean_duration >= goal_duration and episode >= 100:\n",
    "        print('Ran {} episodes. Solved after {} trials'.format(episode, episode - 100))\n",
    "#         agent.test()\n",
    "        env.close()\n",
    "        break\n",
    "\n",
    "    elif episode % 100 == 0:\n",
    "        print('[Episode {}] - Mean time over last 100 episodes was {} frames.'.format(episode, mean_duration))\n",
    "\n",
    "np.save(\"../logs/value/rewards_Q_learning.npy\", all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
